{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cloudpickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = cloudpickle.load(open(\"pickles/cleaned_forum_data.pkl\", \"rb\" ) )\n",
    "\n",
    "totest=df.shape[0]\n",
    "\n",
    "asktxt = df['askertxt'].as_matrix()[:totest]\n",
    "doctxt = df['doctortxt'].as_matrix()[:totest]\n",
    "\n",
    "diag = cloudpickle.load(open(\"pickles/doctortext_labels.pkl\", \"rb\" ) )[:totest,:]\n",
    "y=np.asarray(diag.todense()).squeeze()\n",
    "# y=y[:,y.sum(axis=0)>50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63846, 72)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## import gensim, string\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from collections import namedtuple\n",
    "import string\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "alldocs = []  # will hold all docs in original order\n",
    "alltxts = []\n",
    "for line_no, line in enumerate(asktxt):\n",
    "#     words = my_normalize_nostem(line)\n",
    "    words = line.lower().translate(string.maketrans(string.punctuation, ' '*len(string.punctuation))).split()\n",
    "    tags = [line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
    "    alldocs.append(LabeledSentence(words, tags))\n",
    "    alltxts.append(words)\n",
    "#     alltxts.append([stemmer.stem(t) for t in words])\n",
    "    \n",
    "# for line_no, line in enumerate(doctxt):\n",
    "# #     words = my_normalize_nostem(line)\n",
    "#     words = line.lower().translate(string.maketrans(string.punctuation, ' '*len(string.punctuation))).split()\n",
    "#     tags = [len(asktxt)+line_no] # `tags = [tokens[0]]` would also work at extra memory cost\n",
    "#     alldocs.append(LabeledSentence(words, tags))\n",
    "\n",
    "# doc_list = alldocs[:]  # for reshuffling per pass\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "\n",
    "# my current doc2vec of choice\n",
    "# d2v_vectorizer = Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=3, workers=cores)\n",
    "\n",
    "# #     other options\n",
    "# #     'd2v-sg-concat-200', Doc2Vec(dm=1, dm_concat=1, size=200, window=5, negative=5, hs=0, min_count=2, workers=cores)\n",
    "# #     'd2v-cbow', Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores)\n",
    "# #     'd2v-sg-mean-100', Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores)\n",
    "# #     'd2v-sg-mean-200', Doc2Vec(dm=1, dm_mean=1, size=200, window=10, negative=5, hs=0, min_count=2, workers=cores)\n",
    "\n",
    "# # speed setup by sharing results of 1st model's vocabulary scan\n",
    "# d2v_vectorizer.build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "\n",
    "# %time d2v_vectorizer.train(alldocs)\n",
    "    \n",
    "# vec_represent = %time np.array([d2v_vectorizer.infer_vector(doc.words) for doc in alldocs])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "# # remove_punctuation_map = string.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "# ## First function that creates the tokens\n",
    "# def stem_tokens(tokens):\n",
    "#     return [stemmer.stem(item.translate(remove_punctuation_map)) for item in tokens]\n",
    "\n",
    "# ## Function that incorporating the first function, converts all words into lower letters and removes puctuations maps (previously specified)\n",
    "# def normalize(text):\n",
    "#     return stem_tokens(nltk.word_tokenize(text.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# or use boring vectorizers with different architecture\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "txt_list = np.array(alltxts)\n",
    "tfidf_vec=TfidfVectorizer(analyzer=lambda x: x, stop_words='english', ngram_range=(1,3), max_df=0.8, max_features=10000, min_df=0.01)\n",
    "#     'bagofwords',CountVectorizer(analyzer=lambda x: x, stop_words='english', max_df=0.4, max_features=8000, min_df=0.01)\n",
    "%time tfidf_vec.fit(txt_list)\n",
    "vec_represent=tfidf_vec.transform(txt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "with open(\"pickles/forum_vectorizer.pkl\", 'wb') as handle:\n",
    "    cloudpickle.dump(tfidf_vec, handle)\n",
    "with open(\"pickles/forum_vecrepresent.pkl\", 'wb') as handle:\n",
    "    cloudpickle.dump(vec_represent, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=<function <lambda> at 0x7efbca280b90>, binary=False,\n",
       "        decode_error=u'strict', dtype=<type 'numpy.int64'>,\n",
       "        encoding=u'utf-8', input=u'content', lowercase=True, max_df=0.8,\n",
       "        max_features=10000, min_df=0.01, ngram_range=(1, 3), norm=u'l2',\n",
       "        preprocessor=None, smooth_idf=True, stop_words='english',\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.fit(np.array(['the','boy','was','tall']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x10 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.transform(np.array(['the','boy','was','short']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63846,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_list=np.array(alltxts)\n",
    "txt_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 s, sys: 724 ms, total: 21.5 s\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "vectorized = %time tfidf_vec.fit(txt_list).transform(txt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63846, 892)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = tfidf_vec.transform(np.array([['the','boy','was','short']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 892)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
